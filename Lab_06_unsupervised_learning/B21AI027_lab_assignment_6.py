# -*- coding: utf-8 -*-
"""B21AI027_Lab_Assignment_6

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15AcYjfhwYkiYmlu6TuLP46SP4nT_2u5p

#Question 1

##Initials
"""

import pandas as pd
import numpy as np
import os

if os.path.exists('data.csv'):
    print("Already Present")
else:
    os.system("wget https://www.dropbox.com/s/asxbmdmj7l5tc9j/glass.data")

import warnings
warnings.filterwarnings('ignore')

df=pd.read_csv("glass.data",names=["Id","RI","Na","Mg","Al","Si","K","Ca","Ba","Fe","Type of glass"])

df.drop('Id',axis=1,inplace=True)

df

df.isnull().sum()

#scaling
y=df['Type of glass']
from sklearn.preprocessing import MinMaxScaler
scale=MinMaxScaler()
for col in df.columns:
    df[col]=scale.fit_transform(df[[col]])

df.describe()

import matplotlib.pyplot as plt
import seaborn as sns

sns.pairplot(df,hue='Type of glass')

#df to X,y
X=df.drop('Type of glass',axis=1)

"""##Part A"""

from sklearn.cluster import KMeans
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(X)

labels = kmeans.labels_
centroids = kmeans.cluster_centers_

data = df.copy()

# Applying k-means clustering with 3 clusters
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans.fit(data.iloc[:, 1:10])

#Addon for Visualisation
data['cluster'] = kmeans.labels_

sns.pairplot(data, hue='cluster',palette="husl")
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker='*', s=300, c='r')
plt.show()

"""##Part B

"""

from sklearn.metrics import silhouette_score

k_values = []
silhouette_scores = []

max_sil=0
optimalk=0
for k in range(2, 12):
    kmeans = KMeans(n_clusters=k)
    kmeans.fit(X)
    sil = silhouette_score(X, kmeans.labels_)
    if(sil>max_sil):
        max_sil=sil
        optimalk=k
    k_values.append(k)
    silhouette_scores.append(sil)
    print(f"k={k}, silhouette score={sil}")

# Plot
plt.plot(k_values, silhouette_scores)
plt.xlabel('Number of clusters (k)')
plt.ylabel('Silhouette score')
plt.show()

print(f"Optimalk is {optimalk} ,with silhouette score as {max_sil}")

"""This is because silhouette score is close to 1 when k=3, so 3 is the optimal value of k using silhouette score

##Part C
"""

from yellowbrick.cluster import KElbowVisualizer

model = KMeans()
kelbV = KElbowVisualizer(model, k=(2,10))

kelbV.fit(X)        
kelbV.show()

"""We got the elbow at k=4

##Part D
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

for k in range(1,4):
    knn = KNeighborsClassifier(n_neighbors=k)
    bagging = BaggingClassifier(base_estimator=knn, n_estimators=10, random_state=42)

    bagging.fit(X_train, y_train)
    bag_y_pred_train = bagging.predict(X_train)
    bag_y_pred_test = bagging.predict(X_test)

    knn.fit(X_train, y_train)
    knn_y_pred_train=knn.predict(X_train)
    knn_y_pred_test=knn.predict(X_test)

    accuracy_bag_train = accuracy_score(y_train, bag_y_pred_train)
    accuracy_bag_test = accuracy_score(y_test, bag_y_pred_test)
    accuracy_knn_train = accuracy_score(y_train, knn_y_pred_train)
    accuracy_knn_test = accuracy_score(y_test, knn_y_pred_test)

    print(f"Accuracy score for bagging (k={k}) on train: {accuracy_bag_train}")
    print(f"Accuracy score for bagging (k={k}) on test: {accuracy_bag_test}")
    print(f"Accuracy score for knn (k={k}) on train: {accuracy_knn_train}")
    print(f"Accuracy score for knn (k={k}) on test: {accuracy_knn_test}")
    print()

"""#Question 2

##Initials
"""

from sklearn.datasets import fetch_olivetti_faces
data=fetch_olivetti_faces()

data

print(list(data))

for key, value in data.items():
    if key != 'DESCR':
        print(f'{key}: {value.shape}')

X=data.data
X

y=data.target
y

X.shape

fig, axes = plt.subplots(3, 5, figsize=(10, 6),
                         subplot_kw={'xticks': [], 'yticks': []})

#plotting some image for visualisation
for i, ax in enumerate(axes.flat):
    if i < 5* 5:
        ax.imshow(X[i].reshape(64, 64), cmap='gray')
        ax.set_title(f'Person {y[i]+1}')
plt.tight_layout()
plt.show()

"""##Part A&B"""

# KMeans from scratch
import random
import numpy as np

class KMeans_scratch:
    def __init__(self,n_clusters=40,max_iterations=40000,initial_centroids=[]):
        self.n_clusters=n_clusters
        self.max_iterations=max_iterations
        self.clusters=[]
        self.centroids=[]
        if(len(initial_centroids)!=0):
            self.centroids=initial_centroids
    
    def fit(self,X):
        self.clusters=[[] for _ in range(self.n_clusters)]

        if (len(self.centroids)!=self.n_clusters):
            self.centroids = random.sample(list(X), self.n_clusters)
        # while(len(self.centroids)!=self.n_clusters):
        #     centroi= random.sample(list(X),1)
        #     if(centroi not in self.centroids):
        #         self.centroids.append(centroi)
        # print("centroids",len(self.centroids),len(self.centroids[0]))
        for i in range(self.max_iterations):
            self.clusters=self.assign_clusters(X)
            centroids_old=self.centroids
            # print("cluster",len(self.clusters),len(self.clusters[1]),len(self.clusters[0][0]))
            # print(len(centroids_old[0]))
            self.centroids=self.get_centroids()
            # print(len(self.centroids[0]))
            if(self.is_converged(centroids_old)):
                break
        return self.centroids,self.clusters
 
    def assign_clusters(self,X):
        clusters=[[] for _ in range(self.n_clusters)]
        for point in X:
            # print("1point",len(point))
            closest_centroid_ind=self.closest_centroid(point)
            clusters[closest_centroid_ind].append(point)
        
        for i, cluster in enumerate(clusters):
            if len(cluster) == 0:
                clusters[i] = [random.choice(X)]
        
        # print("len of clusters",len(clusters),len(clusters[0]),len(clusters[0][0]))
        return clusters
    
    def closest_centroid(self,point):
        distances=[]
        for centroid in self.centroids:
            # print("point",len(point))
            # print("centroid,",len(centroid))
            distances.append(np.mean(np.square(point-centroid))**0.5)
        return distances.index(min(distances))
    
    def get_centroids(self):
        centroids=[]
        for i, cluster in enumerate(self.clusters):
            if len(cluster) == 0:
                centroids.append(self.centroids[i])
            else:
                centroid=[]
                for j in range(len(cluster[0])):
                    centroid.append(np.mean([point[j] for point in cluster]))
                centroids.append(np.array(centroid))
        return centroids

    def is_converged(self,centroids_old):
        epsilon=1e-5
        eucli=(np.mean(np.square(np.array(self.centroids)-np.array(centroids_old))))**0.5
        if(eucli<epsilon):
            return True
        return False
        
    def SSE(self):
        sse=0
        for i in range(self.n_clusters):
            for point in self.clusters[i]:
                sse+=np.mean(np.square(point-self.centroids[i]))
        return sse

"""##Part C"""

randompoints_40 = np.random.rand(40, 4096)
model_rand=KMeans_scratch(n_clusters=40,max_iterations=40000,initial_centroids=randompoints_40)
centroids_rand,clusters_rand=model_rand.fit(X)

points_per_cluster = [len(cluster) for cluster in clusters_rand]
print((points_per_cluster))

"""##Part D"""

def array_to_img(array,number_of_images, n_rows=64,n_cols=64):
    numpy_img=np.array(array)
    return numpy_img.reshape(number_of_images, n_rows, n_cols)

centroid_img=array_to_img(centroids_rand,len(centroids_rand))

fig, axs = plt.subplots(5, 8, figsize=(16, 10))
for i, ax in enumerate(axs.flat):
    ax.imshow(centroid_img[i], cmap='gray')
    ax.set_xticks([])
    ax.set_yticks([])
    ax.set_title(f'Cluster {i}')

plt.show()

"""##Part E"""

fig, axs = plt.subplots(5, 8, figsize=(16, 10))
for cluster_ind in range(len(clusters_rand)):
    best_point = clusters_rand[cluster_ind][0]
    for points_ind in range(len(clusters_rand[cluster_ind])):
        if np.linalg.norm(clusters_rand[cluster_ind][points_ind] - centroids_rand[cluster_ind]) < np.linalg.norm(best_point - centroids_rand[cluster_ind]):
            best_point = clusters_rand[cluster_ind][points_ind]
    axs[cluster_ind//8, cluster_ind%8].imshow(np.array(best_point).reshape(64, 64), cmap='gray')
    axs[cluster_ind//8, cluster_ind%8].axis('off')
    axs[cluster_ind//8, cluster_ind%8].set_title("Cluster {}".format(cluster_ind))
    
plt.show()

"""##Part F"""

points_40=[]
X=np.array(X)
for i in range(40):
    random_number = random.randint(0, 9)
    point=X[random_number+i*10,:]
    points_40.append(point)

model_init=KMeans_scratch(n_clusters=40,max_iterations=40000,initial_centroids=points_40)
centroids_init,clusters_init=model_init.fit(X)

points_per_cluster = [len(cluster) for cluster in clusters_init]
print(points_per_cluster)

centroid_img=array_to_img(centroids_init,len(centroids_init))

fig, axs = plt.subplots(5, 8, figsize=(16, 10))
for i, ax in enumerate(axs.flat):
    ax.imshow(centroid_img[i], cmap='gray')
    ax.set_xticks([])
    ax.set_yticks([])
    ax.set_title(f'Cluster {i}')

plt.show()

"""##Part G"""

fig, axs = plt.subplots(5, 8, figsize=(16, 10))
for cluster_ind in range(len(clusters_init)):
    best_point = clusters_init[cluster_ind][0]
    for points_ind in range(len(clusters_init[cluster_ind])):
        if np.linalg.norm(clusters_init[cluster_ind][points_ind] - centroids_init[cluster_ind]) < np.linalg.norm(best_point - centroids_init[cluster_ind]):
            best_point = clusters_init[cluster_ind][points_ind]
    axs[cluster_ind//8, cluster_ind%8].imshow(best_point.reshape(64, 64), cmap='gray')
    axs[cluster_ind//8, cluster_ind%8].axis('off')
    axs[cluster_ind//8, cluster_ind%8].set_title("Cluster {}".format(cluster_ind))
    
plt.show()

"""##Part H"""

print(f'Sum of Squared Error (SSE) for random_initialised points model: {model_rand.SSE()}')
print(f'Sum of Squared Error (SSE) for per_cluster_initialised points model: {model_init.SSE()}')

"""#Question 3

##Part A
"""

#importing libraries
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_moons
from sklearn.preprocessing import MinMaxScaler
from sklearn.cluster import KMeans

if(not(os.path.exists("Wholesalecustomersdata.csv"))):
    os.system("wget https://www.dropbox.com/s/n3cpe2ypk673ih2/Wholesalecustomersdata.csv")

df=pd.read_csv("Wholesalecustomersdata.csv")
df

print(np.unique(df['Channel']))
print(np.unique(df['Region']))

df.isnull().sum()

df.dtypes

scale=MinMaxScaler()
for col in df.columns:
    df[col]=scale.fit_transform(df[[col]])

df.head()

df.describe()

"""##Part B"""

cov_matrix=df.cov()
cov_matrix

plt.figure(figsize=(8,8))
sns.heatmap(cov_matrix, annot=True, cmap='coolwarm')
plt.show()

# Finding best covariance related features
max_cov = 0
for i in range(df.shape[1]):
    for j in range(df.shape[1]):
        if i < j:
            cov_ij = np.cov(df.iloc[:, i], df.iloc[:, j])[0][1]
            if max_cov < cov_ij:
                max_cov = cov_ij

cnt=1
for i in range(df.shape[1]):
    for j in range(df.shape[1]):
        if i < j:
            if(max_cov== np.cov(df.iloc[:, i], df.iloc[:, j])[0][1]):   
                if(cnt):
                    cnt=-1
                    print(np.cov(df.iloc[:, i], df.iloc[:, j]))
                print("Features with max_cov :",[str(df.columns[i]),str(df.columns[j])])

#Visualisation of best_cov feature
plt.scatter(df['Channel'], df['Detergents_Paper'],cmap='BrBG')
plt.xlabel('Channel')
plt.ylabel('Detergents_Paper')
plt.title('Scatter plot of Channel vs Detergents_Paper')
plt.show()

grouped_data = df.groupby('Channel')['Detergents_Paper'].apply(list)
sns.boxplot(data=grouped_data)
plt.xlabel('Channel')
plt.ylabel('Detergents_Paper')
plt.title('Box plot of Detergents_Paper vs Channel')
plt.show()

"""##Part C"""

#DBScan
db = DBSCAN()
clusters = db.fit_predict(df)

# Visualizing
df_temp = df.copy()
df_temp['Cluster'] = clusters

# Pairplot visualization
sns.pairplot(df_temp, hue='Cluster',palette='husl')

"""##Part D"""

kmeans = KMeans(n_clusters=7)
kmeans.fit(df)

df_temp=df.copy()
df_temp['_labels_']=kmeans.labels_
# Visualizing
sns.pairplot(df_temp, hue='_labels_', palette='viridis')
plt.show()

dbscan = DBSCAN()
labels_dbscan = dbscan.fit_predict(df)

kmeans = KMeans(n_clusters=2)
labels_kmeans = kmeans.fit_predict(df)

# Performance
from sklearn.metrics import silhouette_score
silhouette_dbscan = silhouette_score(df, labels_dbscan)
silhouette_kmeans = silhouette_score(df, labels_kmeans)

print(f"DBSCAN Silhouette score: {silhouette_dbscan}")
print(f"KMeans Silhouette score: {silhouette_kmeans}")

"""##Part E"""

X, y = make_moons(n_samples=2000, noise=0.2, random_state=42)

# Visualize the dataset
plt.scatter(X[:, 0], X[:, 1], c=y, cmap='PuOr')
plt.title('Moon dataset with noise')
plt.show()

#DBScan
db = DBSCAN(eps=0.2, min_samples=5)
dbscan_labels = db.fit_predict(X)

#kmeans
kmeans = KMeans(n_clusters=2).fit(X)
kmeans_labels = kmeans.predict(X)

# Visualize the clustering results using DBSCAN
plt.scatter(X[:, 0], X[:, 1], c=dbscan_labels, cmap='Set2')
plt.title('DBSCAN clustering')
plt.show()

# Visualize the clustering results using KNN
plt.scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='BrBG')
plt.title('Kmeans clustering')
plt.show()

#DBScan with 
db = DBSCAN(eps=0.2, min_samples=50)
dbscan_labels = db.fit_predict(X)

# Visualize the clustering results using DBSCAN
plt.scatter(X[:, 0], X[:, 1], c=dbscan_labels, cmap='Set2')
plt.title('DBSCAN clustering')
plt.show()

