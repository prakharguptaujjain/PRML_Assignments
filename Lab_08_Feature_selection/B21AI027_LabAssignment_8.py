# -*- coding: utf-8 -*-
"""B21AI027_LabAssignment_8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Hj3vjBqgeQEYTckT_ErpjoNd5ATJc_D7

#Question 1

##Part 1
"""

!pip install mlxtend

import joblib
import sys
sys.modules['sklearn.externals.joblib'] = joblib
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
import pandas as pd
import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier

if os.path.exists('anneal_names.txt'):
    print("Already Present")
else:
    os.system("wget https://www.dropbox.com/s/24t8jhmde1bzjmt/train.csv")

np.random.seed(42)

df=pd.read_csv("train.csv")

df

df.isnull().sum()

df.dropna(inplace=True)
df.drop(['Unnamed: 0'],axis=1,inplace=True)
df.drop(['id'],axis=1,inplace=True)

df.dtypes

for i in df.columns:
    print(len(np.unique(df[i])))

col_to_encode=['Gender', 'Customer Type', 'Type of Travel', 'Class', 'satisfaction']

for i in col_to_encode:
    encoder=LabelEncoder()
    df[i]=encoder.fit_transform(df[i])
    print(f"{i} encoding: {dict(zip(encoder.classes_, range(len(encoder.classes_))))}")

#Scalling the dataset
scaler = MinMaxScaler()
for col in df.columns:
    if(col!='satisfaction'):
        df[col] = scaler.fit_transform(np.array(df[col]).reshape(len(df[col]),1))

df

df.describe()

#df to X,y
X=df.drop('satisfaction',axis=1)
y=df['satisfaction']

len(X.columns)

"""##Part 2"""

dt = DecisionTreeClassifier()
sfs = SFS(dt, k_features=10, forward=True, floating=False, scoring='accuracy')
sfs.fit(X, y)

for subset in sfs.subsets_.keys():
    print("Subset:", subset)
    print("Accuracy Score:", sfs.subsets_[subset]['avg_score'])
    print("------------------------------")

# Names of the features selected
print("Best 10 features selected by SFS:")
for feature_idx in sfs.k_feature_idx_:
    print(X.columns[feature_idx])

"""##Part 3"""

# SFS (forward=True, floating=False)
dt_sfs = DecisionTreeClassifier()
sfs = SFS(dt_sfs, k_features=10, forward=True, floating=False, scoring='accuracy',cv=4)
sfs.fit(X, y)

for k, v in sfs.subsets_.items():
    print("Subset:", k)
    print("cv_scores:", v['cv_scores'])

print(sfs.subsets_)

# SBS (forward=False, floating=False)
dt_sbs = DecisionTreeClassifier()
sbs = SFS(dt_sbs, k_features=10, forward=False, floating=False, scoring='accuracy',cv=4)
sbs.fit(X.values, y.values)

for k, v in sbs.subsets_.items():
    print("Subset:", k)
    print("cv_scores:", v['cv_scores'])

print(sbs.subsets_)

# SFFS (forward=True, floating=True)
dt_sffs = DecisionTreeClassifier()
sffs = SFS(dt_sffs, k_features=10, forward=True, floating=True, scoring='accuracy',cv=4)
sffs.fit(X.values, y.values)

for k, v in sffs.subsets_.items():
    print("Subset:", k)
    print("cv_scores:", v['cv_scores'])

print(sffs.subsets_)

# SBFS (forward=False, floating=True)
dt_sbfs = DecisionTreeClassifier()
sbfs = SFS(dt_sbfs, k_features=10, forward=False, floating=True, scoring='accuracy',cv=4)
sbfs.fit(X.values, y.values)

for k, v in sbfs.subsets_.items():
    print("Subset:", k)
    print("cv_scores:", v['cv_scores'])

print(sbfs.subsets_)

sfs_results = pd.DataFrame.from_dict(sfs.get_metric_dict()).T
sbs_results = pd.DataFrame.from_dict(sbs.get_metric_dict()).T
sffs_results = pd.DataFrame.from_dict(sffs.get_metric_dict()).T
sbfs_results = pd.DataFrame.from_dict(sbfs.get_metric_dict()).T

print("SFS Results:\n", sfs_results)
print("SBS Results:\n", sbs_results)
print("SFFS Results:\n", sffs_results)
print("SBFS Results:\n", sbfs_results)

"""##Part 4"""

from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs

print("SFS")
plot_sfs(sfs.get_metric_dict(), kind='std_err')

print("SBS")
plot_sfs(sbs.get_metric_dict(), kind='std_err')

print("SFFS")
plot_sfs(sffs.get_metric_dict(), kind='std_err')

print("SBFS")
plot_sfs(sbfs.get_metric_dict(), kind='std_err')

"""##Part 5"""

from sklearn.feature_selection import mutual_info_classif
from scipy.spatial.distance import pdist, squareform
from sklearn.impute import SimpleImputer
from sklearn.metrics.pairwise import cosine_similarity

def bi_directional_feature_selection(X, y, selection_criterion=None, distance_measure=None):
    # F1 = all features and F2 as empty (initialise)
    F1 = set(range(X.shape[1]))
    F2 = set()
    
    # distance matrix
    def distance_matrix_func(X_subset, y):
        if distance_measure == 'angular':
            return 1
        if distance_measure == 'euclidean':
            return pdist(X_subset, metric='euclidean')
        elif distance_measure == 'cityblock':
            return pdist(X_subset, metric='cityblock')
            
    # via accuracy_measure
    if selection_criterion == 'decision_tree':
        def selection_criterion_func(X_subset, y):
            dt = DecisionTreeClassifier()
            dt.fit(X_subset, y)
            return dt.score(X_subset, y)
    elif selection_criterion == 'svm':
        def selection_criterion_func(X_subset, y):
            svm = SVC()
            svm.fit(X_subset, y)
            return svm.score(X_subset, y)
    elif selection_criterion == 'info_gain':
        selection_criterion_func = lambda X_subset, y: mutual_info_classif(X_subset, y)
    else:                                                                       #selection_criterion == 'distance'
        def selection_criterion_func(X_subset, y,dist_func):
            dt = DecisionTreeClassifier()
            dt.fit(X,y)
            if distance_measure == 'angular':              
                similarity_matrix = cosine_similarity(X)
                score = similarity_matrix[np.triu_indices(similarity_matrix.shape[0], k=1)].mean()
                return score
            else :
                distance_matrix = squareform(dist_func)
            scores = cross_val_score(dt, distance_matrix, y, cv=2)
            return scores.mean()


    def criteria_or_distance_value(X_subset, y):
        if distance_measure is None:
            return np.mean(selection_criterion_func(X_subset, y))
        else:
            return np.mean(selection_criterion_func(X_subset,y,distance_matrix_func(X_subset, y)))


    # iterate until no more features can be added or removed
    bestie_score=[]
    while True:
        # print("size of F1: ", len(F1), "size of F2: ", len(F2))
        # forward step: remove one feature at a time from F1 and add it to F2
        curr_score = 0
        if F2 != set():
            curr_score = criteria_or_distance_value(X[:, list(F2)], y)
        # print("current score: ", curr_score)
        bestie_score.append(curr_score)

        scores_forward = []
        if F1 != set():
            for f in F1:
                F2_temp = F2.union({f})
                X_subset = X[:, list(F2_temp)]
                score = criteria_or_distance_value(X_subset, y)
                scores_forward.append((score, f))
            # backward step: remove one feature at a time from F2 and add it to F1
        scores_backward = []

        #F2 should have single feature
        if F2 != set():
            for f in F2:
                F2_temp = F2.difference({f})
                if F2_temp == set():
                    break
                X_subset = X[:, list(F2_temp)]
                score = criteria_or_distance_value(X_subset, y)
                scores_backward.append((score, f))


        # find the best score for forward and backward steps
        if F1 != set():
            best_forward = np.max(scores_forward, axis=0)
            best_forward_score = best_forward[0]
            best_forward_feature = int(best_forward[1])
            # print(best_forward_score, best_forward_feature)

        if F2 != set() and F2_temp != set():
            best_backward = np.max(scores_backward, axis=0)
            best_backward_score = best_backward[0]
            best_backward_feature = int(best_backward[1])
            # print(best_backward_score, best_backward_feature)

        best_of_all_score=curr_score
        if(F1 != set()):
            best_of_all_score = max(best_of_all_score, best_forward_score)
        if(F2 != set() and F2_temp != set()):
            best_of_all_score = max(best_of_all_score, best_backward_score)
        # best score for the current iteration
        if((F1 != set() and curr_score >= best_forward_score) and (F2 != set() and F2_temp != set() and curr_score >= best_backward_score)):
            break
        elif(F1 != set() and best_forward_score == best_of_all_score):
            F1= F1.difference({best_forward_feature})
            F2 = F2.union({best_forward_feature})
        elif(F2 != set() and F2_temp != set() and best_backward_score == best_of_all_score):
            F2 = F2.difference({best_backward_feature})
            F1 = F1.union({best_backward_feature})
        # if(len(bestie_score)>4):
        #     print(bestie_score)
        if(len(bestie_score)>16 and np.mean(bestie_score[-int(len(bestie_score)/2):])==np.mean(bestie_score[-int(len(bestie_score)/4):])):
            break
        if len(F1) == 0 or len(F2) == 0:
            break

    return list(F2)

"""##Part 6"""

X_preserved=X.copy()
y_preserved=y.copy()

#Decreasing data size as it is taking lot of time to train
from sklearn.model_selection import StratifiedShuffleSplit
strat_split = StratifiedShuffleSplit(n_splits=1, test_size=1000, random_state=42)
_, sample_idx = next(strat_split.split(X_preserved, y_preserved))
X = X_preserved.iloc[sample_idx,:]
y = y_preserved.iloc[sample_idx]

print(X.shape)
print(y.shape)

# Accuracy Measure using SVM Classifier
feature_set = bi_directional_feature_selection(X.values, y.values,selection_criterion='svm')
X_train, X_test, y_train, y_test = train_test_split(X.values[:, feature_set], y.values, test_size=0.2, random_state=42)
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)
accuracy = dt.score(X_test, y_test)
print("Accuracy score using Decision Tree (metric - Accuracy Measure using SVM Classifier):", accuracy)

# Information Measures: Information gain
feature_set = bi_directional_feature_selection(X.values, y.values,selection_criterion='info_gain')
X_train, X_test, y_train, y_test = train_test_split(X.values[:, feature_set], y.values, test_size=0.2, random_state=42)
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)
accuracy = dt.score(X_test, y_test)
print("Accuracy score using Decision Tree (metric - Information Measures: Information gain):", accuracy)

# Distance Measure: City-Block Distance
feature_set = bi_directional_feature_selection(X.values, y.values,distance_measure='cityblock')
X_train, X_test, y_train, y_test = train_test_split(X.values[:, feature_set], y.values, test_size=0.2, random_state=42)
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)
accuracy = dt.score(X_test, y_test)
print("Accuracy score using Decision Tree (metric - Distance Measure: City-Block Distance:)", accuracy)

"""#Question 2

##Part 1
"""

np.random.seed(42)

cov_matrix = np.array([[0.6006771, 0.14889879, 0.244939],
                       [0.14889879, 0.58982531, 0.24154981],
                       [0.244939, 0.24154981, 0.48778655]])

#v
v = np.array([1/np.sqrt(6), 1/np.sqrt(6), -2/np.sqrt(6)]).reshape(-1,1)

# Generate data
n_points = 1000
data = np.random.multivariate_normal(mean=np.zeros(3), cov=cov_matrix, size=n_points)

# labelling
labels = np.where(np.dot(data, v) > 0, 0, 1)

import plotly.graph_objs as go

trace = go.Scatter3d(x=data[:,0], y=data[:,1], z=data[:,2],
                     mode='markers', marker=dict(color=labels))

layout = go.Layout(title='3D Scatter Plot')

# Plottinh
fig = go.Figure(data=[trace], layout=layout)
fig.show()

"""##Part 2"""

from sklearn.decomposition import PCA
pca = PCA(n_components=3)
data_transformed = pca.fit_transform(data)

"""##Part 3"""

from sklearn.tree import DecisionTreeClassifier
import itertools

num_features = 2
feature_combinations = list(itertools.combinations(range(3), num_features))

for feature_pair in feature_combinations:
    X_subset = data_transformed[:, feature_pair]

    clf = DecisionTreeClassifier()
    clf.fit(X_subset, y)
    
    # Plotting
    plt.figure()
    plt.scatter(X_subset[:, 0], X_subset[:, 1], c=y)
    plt.title(f'Decision Boundary for Features {feature_pair}')
    plt.xlabel('Principal Component 1')
    plt.ylabel('Principal Component 2')
    
    # Plotting
    x_min, x_max = X_subset[:, 0].min() - 0.1, X_subset[:, 0].max() + 0.1
    y_min, y_max = X_subset[:, 1].min() - 0.1, X_subset[:, 1].max() + 0.1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),
                         np.arange(y_min, y_max, 0.01))
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    #decision boundary
    plt.contourf(xx, yy, Z, alpha=0.5)
    plt.show()

"""##Part 4"""

cov_mat = np.cov(data_transformed.T)

# plotting
sns.heatmap(cov_mat, annot=True, cmap='coolwarm')
plt.title('Covariance Matrix of X')
plt.show()

X=data_transformed
y=labels

# Features (0,2)
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

X_train, X_test, y_train, y_test = train_test_split(X[:, [0,2]], y, test_size=0.2)

# Fit
tree = DecisionTreeClassifier()
tree.fit(X_train, y_train)

accuracy = tree.score(X_test, y_test)
print('Accuracy:', accuracy)

# Features (1,2)
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

X_train, X_test, y_train, y_test = train_test_split(X[:, [1,2]], y, test_size=0.2)

# Fit
tree = DecisionTreeClassifier()
tree.fit(X_train, y_train)

accuracy = tree.score(X_test, y_test)
print('Accuracy:', accuracy)

# Features (0,1)
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

X_train, X_test, y_train, y_test = train_test_split(X[:, [0,1]], y, test_size=0.2)

# Fit
tree = DecisionTreeClassifier()
tree.fit(X_train, y_train)

accuracy = tree.score(X_test, y_test)
print('Accuracy:', accuracy)