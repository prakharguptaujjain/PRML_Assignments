# -*- coding: utf-8 -*-
"""B21AI027_LabAssignment_9.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12s40Qgh8dU2HZZ9WpMQ8nnm__6MX_x_d

#Question 1

##Part A
"""

import torch
from torchvision import datasets, transforms
import numpy as np
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import torch.optim as optim

train_transforms = transforms.Compose([
    transforms.RandomRotation(5),
    transforms.RandomCrop(28, padding=2),
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

test_transforms = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

# MNIST dataset
train_dataset = datasets.MNIST('./data', train=True, download=True, transform=train_transforms)
test_data = datasets.MNIST('./data', train=False, download=True, transform=test_transforms)

train_data, val_data = train_test_split(train_dataset, test_size=0.2, random_state=42)

"""##Part B"""

# Plottin images
fig, axs = plt.subplots(10, 10, figsize=(10, 10))
for i in range(10):
    idxs = np.where(train_dataset.targets == i)[0][:10]
    for j, idx in enumerate(idxs):
        img, label = train_dataset[idx]
        axs[i, j].imshow(img.squeeze(), cmap='gray')
        axs[i, j].axis('off')
plt.show()

from torch.utils.data import DataLoader

batch_size = 64

train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)

# shape of the data
for data, target in train_loader:
    print(data.shape)
    print(target.shape)
    break

input_size=28*28

"""##Part C"""

import torch.nn as nn

class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(28*28, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 10)

    def forward(self, x):
        x = x.view(-1, self.fc1.in_features)  # Flatten input
        x = nn.functional.relu(self.fc1(x))
        x = nn.functional.relu(self.fc2(x))
        x = self.fc3(x)
        return x

model = MLP()
print(f"Number of trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}")

"""##Part D"""

train_loss_arr=[]
train_acc_arr=[]
val_loss_arr=[]
val_acc_arr=[]

def train(model, train_loader, val_loader, optimizer, criterion, epochs=5):
    best_val_acc = 0
    for epoch in range(epochs):
        model.train()
        train_loss = 0
        train_acc = 0
        for data, target in train_loader:
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
            train_acc += (output.argmax(1) == target).sum().item()
        train_loss /= len(train_loader)
        train_acc /= len(train_dataset)
        val_loss = 0
        val_acc = 0
        model.eval()
        with torch.no_grad():
            for data, target in val_loader:
                output = model(data)
                loss = criterion(output, target)
                val_loss += loss.item()
                val_acc += (output.argmax(1) == target).sum().item()
        val_loss /= len(val_loader)
        val_acc /= len(val_data)
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), 'best_model.pt')
        print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.3F}, Train Acc: {train_acc:.3F}, Val Loss: {val_loss:.3F}, Val Acc: {val_acc:.3F}')
        train_loss_arr.append(train_loss)
        train_acc_arr.append(train_acc)
        val_loss_arr.append(val_loss)
        val_acc_arr.append(val_acc)
        
optimizer = optim.Adam(model.parameters())
criterion = nn.CrossEntropyLoss()
train(model, train_loader, val_loader, optimizer, criterion,5)

"""##Part E"""

model.load_state_dict(torch.load('best_model.pt'))
model.eval()
correct = []
incorrect = []
with torch.no_grad():
    for data, target in test_loader:
        output = model(data)
        pred = output.argmax(1)
        correct.extend([(data[i], target[i], pred[i]) for i in range(len(pred)) if pred[i] == target[i]])
        incorrect.extend([(data[i], target[i], pred[i]) for i in range(len(pred)) if pred[i] != target[i]])
        if len(correct) == 25 and len(incorrect) == 25:
            break

print("Correct Predictions")
fig, axs = plt.subplots(5, 5, figsize=(10, 10))
fig.subplots_adjust(wspace=0.4, hspace=0.6)
for i in range(5):
    for j in range(5):
        img, actual, pred = correct[i*5 + j]
        axs[i, j].imshow(img.squeeze(), cmap='gray')
        axs[i, j].set_title("Actual: {} \n Predicted: {}".format(actual.item(), pred.item()))
        axs[i, j].axis('off')
plt.show()

print("Incorrect Predictions")
fig, axs = plt.subplots(5, 5, figsize=(10, 10))
fig.subplots_adjust(wspace=0.4, hspace=0.6)
for i in range(5):
    for j in range(5):
        img, actual, pred = incorrect[i*5 + j]
        axs[i, j].imshow(img.squeeze(), cmap='gray')
        axs[i, j].set_title("Actual: {} \n Predicted: {}".format(actual.item(), pred.item()))
        axs[i, j].axis('off')
plt.show()

epochs = range(1, 6)

# Plot Loss vs. Epoch for Training and Validation
plt.plot(epochs, train_loss_arr, label='Train Loss')
plt.plot(epochs, val_loss_arr, label='Val Loss')
plt.title('Loss vs. Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.xticks(epochs)
plt.legend()
plt.show()

# Plot Accuracy vs. Epoch for Training and Validation
plt.plot(epochs, train_acc_arr, label='Train Acc')
plt.plot(epochs, val_acc_arr, label='Val Acc')
plt.title('Accuracy vs. Epoch')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.xticks(epochs)
plt.legend()
plt.show()

"""#Question 2

##Part A
"""

import joblib
import sys
sys.modules['sklearn.externals.joblib'] = joblib
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
import pandas as pd
from sklearn.model_selection import train_test_split
import os
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

if os.path.exists('abalone.data'):
    print("Already Present")
else:
    os.system("wget https://www.dropbox.com/s/jfiypwq4vxvfy58/abalone.data")

np.random.seed(2)

column_names = ["Sex", "Length", "Diameter", "Height", "Whole weight", "Shucked weight", "Viscera weight", "Shell weight", "Rings"]
df = pd.read_csv("abalone.data", names=column_names)

df

df.isnull().sum()

np.unique(df['Rings'])

df.dtypes

np.unique(df['Sex'])

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
df["Sex"] = encoder.fit_transform(df["Sex"])

# Print the conversion
print("Conversion for Sex column:")
for i, label in enumerate(encoder.classes_):
    print(f"{label} -> {i}")


encoder = LabelEncoder()
df["Rings"] = encoder.fit_transform(df["Rings"])

df

df.describe()

import seaborn as sns
import pandas as pd

# Bar Plots
for column in df.columns[:-1]:  
    plt.figure()
    if column in ["Length", "Diameter", "Height", "Whole weight", "Shucked weight", "Viscera weight", "Shell weight"]:
        sns.barplot(x=pd.cut(df[column], bins=10), y="Rings", data=df)
    else:
        sns.barplot(x=column, y="Rings", data=df)
    plt.xlabel(column)
    plt.ylabel("Rings")
    plt.show()

sns.histplot(data=df, x="Rings")

# correlation plot
corr = df.corr()
sns.heatmap(corr, annot=True)
plt.show()

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

# standardization
for col in df.columns[:-1]:
    df[col] = scaler.fit_transform(df[[col]])

unique_values, counts = np.unique(df["Rings"], return_counts=True)
print("Unique_Values:",unique_values)
print("Counts:",counts)

df.describe()

# threshold for pre-taking class and later adding to train
df_preserved = df.copy(deep=True)
threshold = 3
class_counts = df["Rings"].value_counts()
df_temp = pd.DataFrame()

for cls, count in class_counts.items():
    if count < threshold:
        df_temp = df_temp.append(df[df["Rings"]==cls])

# Drop the rows
df = df.drop(df_temp.index)

# TTS with stratified sampling
X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df["Rings"], test_size=0.2, stratify=df["Rings"], random_state=2)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=2)

# Merge df_temp and X_train
X_train = pd.concat([X_train, df_temp.drop(["Rings"], axis=1)])
y_train = pd.concat([y_train, df_temp["Rings"]])

df_temp

X_train

y_train

unique_values, counts = np.unique(y_train, return_counts=True)
print("Class distribution in train set:")
for val, count in zip(unique_values, counts):
    print(f"{val}: {count/len(y_train):.2%}")
    
print()

unique_values, counts = np.unique(y_test, return_counts=True)
print("Class distribution in test set:")
for val, count in zip(unique_values, counts):
    print(f"{val}: {count/len(y_test):.2%}")

num_labels = len(np.unique(df_preserved["Rings"]))
print(num_labels)

# Apply one-hot encoding
y_one_hot_train = np.eye(num_labels)[np.array(y_train).astype(int).reshape(1,-1)]
y_reshaped_train = y_one_hot_train.reshape(-1, num_labels)

y_one_hot_test = np.eye(num_labels)[np.array(y_test).astype(int).reshape(1,-1)]
y_reshaped_test = y_one_hot_test.reshape(-1, num_labels)

y_one_hot_val = np.eye(num_labels)[np.array(y_val).astype(int).reshape(1,-1)]
y_reshaped_val = y_one_hot_val.reshape(-1, num_labels)

"""##Part B

"""

import pickle
class ANN_scratch:
    def __init__(self, input_dim, hidden_layers,output_dim,layer_dim,activation,batch_size=100,learning_rate=0.01,weight_init='random',onebynthweight_value=8,gradient_clip=0.5):
        self.input_dim = input_dim
        self.hidden_layers = hidden_layers
        self.learning_rate = learning_rate
        self.preserved_learning_rate = learning_rate
        self.activations=activation
        self.batch_size=batch_size
        self.weight_init=weight_init
        self.gradien_clip=gradient_clip
        self.onebynthweight_value=onebynthweight_value
        self.layer_dim=[]
        self.layer_dim.append(input_dim)
        for i in range(self.hidden_layers):
            self.layer_dim.append(layer_dim[i])
        self.layer_dim.append(output_dim)
        
        self.layers={}
        self.layers_init()
    def layers_init(self):
        for i in range(1,self.hidden_layers+1):
            parametrs={}
            if self.weight_init=='random':
                weights=np.random.randn(self.layer_dim[i],self.layer_dim[i-1])
            elif self.weight_init=='zero':
                weights=np.zeros((self.layer_dim[i],self.layer_dim[i-1]))
            elif self.weight_init=='constant':
                weights=np.ones((self.layer_dim[i],self.layer_dim[i-1]))/self.onebynthweight_value
            bias=np.random.randn(self.layer_dim[i],1)
            activation=self.activations[i-1]
            parametrs['weights']=weights
            parametrs['bias']=bias
            parametrs['activation']=activation
            self.layers[i-1]=parametrs
            # print("Parametrs of layer ",i-1,": ",parametrs)

    def activation(self, activation_type, x):
        if (activation_type == 'sigmoid'):
            return 1 / (1 + np.exp(-x))
        elif (activation_type == 'tanh'):
            return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))
        elif (activation_type == 'relu'):
            return np.maximum(0, x)
        elif (activation_type == 'leaky_relu'):
            return np.where(x > 0, x, x * 0.01)
        elif (activation_type == 'softmax'):
            e_x = np.exp(x - np.max(x, axis=1, keepdims=True))
            return e_x / np.sum(e_x, axis=1, keepdims=True)
        elif (activation_type == 'linear'):
            return x
        
    def loss(self, loss_fxn, y, y_hat):
        if (loss_fxn == 'mse'):
            return np.mean(np.square(y - y_hat)), y_hat
        elif (loss_fxn == 'cross_entropy'):
            # add 1e-8 to avoid log(0)
            y_hat = np.clip(y_hat, 1e-8, 1 - 1e-8)
            loss = -np.mean(y * np.log(y_hat), axis=1)
            return loss, y_hat


    def loss_fxn_da(self, loss_fxn, y, y_hat):
        if loss_fxn == 'mse':
            return -2 * (y - y_hat)
        elif loss_fxn == 'cross_entropy':
            return (y_hat - y) / (y_hat * (1 - y_hat))

        
    def activation_derivative(self, activation_type, z):
        if activation_type == 'sigmoid':
            return self.activation('sigmoid', z) * (1 - self.activation('sigmoid', z))
        elif activation_type == 'tanh':
            return 1 - self.activation('tanh', z)**2
        elif activation_type == 'relu':
            return np.where(z > 0, 1, 0)
        elif activation_type == 'leaky_relu':
            return np.where(z > 0, 1, 0.01)
        elif activation_type == 'linear':
            return 1
        elif activation_type == 'softmax':
            s = self.activation('softmax', z)
            return s * (1 - s)

    def forward_propagate(self, a_prev, layer_number):
        layer = self.layers[layer_number]
        weights = layer['weights']
        bias = layer['bias']
        activation_type = layer['activation']
        z = np.dot(weights, a_prev) + bias
        a = self.activation(activation_type, z)
        return a, z
    
    def backward_propagate(self, za, zs, y, y_hat,X,loss_fxn):
        da = self.loss_fxn_da(loss_fxn, y, y_hat)
        m=X.shape[1]
        for i in range(len(self.layers) - 1, -1, -1):
            layer = self.layers[i]
            weights = layer['weights']
            activation_type = layer['activation']
            z = zs[i]
            a = za[i]
            # if activation_type == 'softmax' and i==len(self.layers) - 1:
            #     dz =  self.activation_derivative(activation_type, z) * da
            # else:
            # print("weights:",weights.shape)
            # print("da:",da.shape)
            if i != len(self.layers) - 1:
                # print("self.layers[i+1]['weights']:",self.layers[i+1]['weights'].shape)
                # print("da:",da.shape)
                da= np.dot(self.layers[i+1]['weights'].T, dz)
            dz = self.activation_derivative(activation_type, z) * da
            # print("dz:",dz.shape)
            # print("a:",a.shape)
            if(i==0):
                a_prev=X
            else:
                a_prev=za[i-1]
            self.update_parameters(i, dz, a_prev,self.learning_rate,m, clip_value=self.gradien_clip)
        
    def update_parameters(self, layer_number, dz, a_prev, learning_rate, m, clip_value):
        weights = self.layers[layer_number]['weights']
        bias = self.layers[layer_number]['bias']
        
        # Calculate the gradients with respect to the weights and bias
        dW = np.dot(dz, a_prev.T) / m
        db = np.sum(dz, axis=1, keepdims=True) / m
        
        # Clip the gradients to the given value
        dW = np.clip(dW, -clip_value, clip_value)
        db = np.clip(db, -clip_value, clip_value)
        
        # Update the weights and bias using the clipped gradients
        weights -= learning_rate * dW
        bias -= learning_rate * db
        
        self.layers[layer_number]['weights'] = weights
        self.layers[layer_number]['bias'] = bias


    def single_pass_loss(self,x_train, y_train,m,loss_fxn):
        cnt = 0
        train_loss = []
        while cnt * self.batch_size < m:
            x_batch = x_train[:, cnt * self.batch_size : (cnt + 1) * self.batch_size]
            y_batch = y_train[:, cnt * self.batch_size : (cnt + 1) * self.batch_size]

            a = x_batch
            zs = []
            za = []
            for j in range(self.hidden_layers):
                a, z = self.forward_propagate(a, j)
                zs.append(z)
                za.append(a)
            y_hat = a
            # add 1e-8 to avoid log(0)
            y_hat = np.clip(y_hat, 1e-8, 1 - 1e-8)
            loss, _ = self.loss(loss_fxn, y_batch, y_hat)
            train_loss.append(loss)
            self.backward_propagate(za, zs, y_batch, y_hat, x_batch, loss_fxn)
            cnt += 1
        
        return np.mean(train_loss)

    def train(self, x_train, y_train, x_val,y_val, epochs, loss_fxn,decay_rate=0.01, validation_split=0.2,rst=0.1,decay_loop=25):
        m = y_train.shape[1]
        history = {}
        train_loss_hist=[]
        val_loss_hist=[]
        train_acc_hist=[]
        val_acc_hist=[]

        for epoch in range(epochs):
            cnt = 0
            train_loss = []
            while cnt * self.batch_size < m:
                x_batch = x_train[:, cnt * self.batch_size : (cnt + 1) * self.batch_size]
                y_batch = y_train[:, cnt * self.batch_size : (cnt + 1) * self.batch_size]

                a = x_batch
                zs = []
                za = []
                for j in range(self.hidden_layers):
                    a, z = self.forward_propagate(a, j)
                    zs.append(z)
                    za.append(a)
                y_hat = a
                # add 1e-8 to avoid log(0)
                y_hat = np.clip(y_hat, 1e-8, 1 - 1e-8)
                loss, _ = self.loss(loss_fxn, y_batch, y_hat)
                train_loss.append(loss)
                self.backward_propagate(za, zs, y_batch, y_hat, x_batch, loss_fxn)
                cnt += 1

            train_loss = np.mean(train_loss)


            if epoch % decay_loop == 0:
                self.learning_rate = self.preserved_learning_rate* np.exp(-decay_rate * epoch)

            #store train_loss and train_acc
            if loss_fxn =='cross_entropy':
                if np.isnan(np.mean(loss)) and epoch %15==0 and epoch > 0:
                  for i in range(5):
                    if (not(np.isnan(train_loss))):
                      break
                    self.layers_init()
                    for i in range(epoch):
                      train_loss=self.single_pass_loss(x_train, y_train,m,loss_fxn)
                train_loss_hist.append(train_loss)
                train_acc_hist.append(self.accuracy(x_train,y_train,loss_fxn))
            elif loss_fxn=='mse':
                train_loss_hist.append(train_loss)
                val_acc_hist.append(self.accuracy(x_val,y_val,loss_fxn))  

            #store val_loss and val_acc
            if loss_fxn =='cross_entropy':
                loss, _ =self.loss(loss_fxn, y_val,self.predict(x_val))
                if np.isnan(np.mean(loss)):
                  for i in range(5):
                    if(not(np.isnan(train_loss))) and epoch %15==0 and epoch > 0:
                      break
                    self.layers_init()
                    for i in range(epoch):
                      train_loss=self.single_pass_loss(x_train, y_train,m,loss_fxn)
                val_acc_hist.append(self.accuracy(x_val,y_val,loss_fxn))
                val_loss_hist.append(np.mean(loss))
            elif loss_fxn=='mse':
                val_loss_hist.append(np.mean(loss))
                val_acc_hist.append(self.accuracy(x_val,y_val,loss_fxn))
            
            if epoch % 10 == 0:
                if np.isnan(train_loss):
                    print("Epoch: ", (epoch+1), " Train Loss: ")
                else:
                    if loss_fxn =='cross_entropy':
                        loss, _ =self.loss(loss_fxn, y_val,self.predict(x_val))
                        print("Epoch: ", (epoch+1), " Train Loss: ", train_loss, " Val loss: ", np.mean(loss))
                        print("Epoch: ", (epoch+1), " Train acc: ", train_acc_hist[-1], " Val acc: ", val_acc_hist[-1])

                    elif loss_fxn=='mse':
                        print("Epoch: ", (epoch+1), " Train Loss: ", train_loss, " Val loss: ", self.accuracy(x_val,y_val,'mse'))
            

            #Weight reset
            wei_rst=0
            if epoch % 25==0 and epoch>0:
                if(train_loss>rst):
                    wei_rst=1
                    self.preserved_learning_rate /= 2
                    self.learning_rate =self.preserved_learning_rate 
                    print("Weights reset",end=" ")
                    for i in range(5):
                      if (train_loss < rst):
                        break
                      print(i+1,end=" ")
                      self.layers_init()
                      for i in range (epoch):
                          train_loss=self.single_pass_loss(x_train, y_train,m,loss_fxn)
                      print("Debug train_acc: ",self.accuracy(x_train,y_train,loss_fxn),end=" ")
            if wei_rst:
              print()
            # Check convergence
            if epoch > 0 and epoch % 25 == 0 and wei_rst==0:
                if abs(train_loss_hist[-1] - train_loss_hist[-2]) < train_loss_hist[-2]*1e-6 and epoch > 10:
                    print(f"Converged after {epoch} epochs.")
                    # print(f"Accuracy at {epoch} epochs is {np.mean(batch_loss)}")
                    break
        history={'train_loss':train_loss_hist,'val_loss':val_loss_hist,'train_acc':train_acc_hist,'val_acc':val_acc_hist}
        return history


    def predict(self,x):
        a=x
        for j in range(self.hidden_layers):
            a, z = self.forward_propagate(a, j)
        return a
    
    def accuracy(self,x,y,loss_fxn):
        if loss_fxn=='mse':
            y_hat=self.predict(x)
            loss, _ = self.loss(loss_fxn, y, y_hat)
            return np.mean(loss)
        elif loss_fxn=='cross_entropy':
            y_hat=self.predict(x)
            y_hat=np.argmax(y_hat,axis=0)
            y=np.argmax(y,axis=0)
            return np.sum(y_hat==y)/y.shape[0]
    def save_weights(self, filename):
        with open(filename, 'wb') as f:
            pickle.dump(self.layers, f)
    
    def load_weights(self, filename):
        with open(filename, 'rb') as f:
            self.layers = pickle.load(f)
            self.layers['activation']=self.activations

"""##Part C"""

#Sigmoid
layer_dim=[512,128,64,num_labels]
activation=['sigmoid','sigmoid','sigmoid','sigmoid']
model=ANN_scratch(8,4,num_labels,layer_dim,activation,batch_size=16,learning_rate=0.001)
history=model.train(X_train.values.T,y_reshaped_train.T,X_val.values.T,y_reshaped_val.T,501,'cross_entropy',decay_rate=0.001,rst=0.1,decay_loop=75)
print(f"Accuracy on test data: {model.accuracy(X_test.values.T,y_reshaped_test.T,'cross_entropy')}")

#train and validation loss over epoch
plt.plot(history['train_loss'], label='Training Loss')
plt.plot(history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

#training and alidation accuracy over epoch
plt.plot(history['train_acc'], label='Training Accuracy')
plt.plot(history['val_acc'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Tanh activation function
layer_dim=[512,128,64,num_labels]
activation=['tanh','tanh','tanh','softmax']
model=ANN_scratch(8,4,num_labels,layer_dim,activation,batch_size=16,learning_rate=0.01)
history=model.train(X_train.values.T,y_reshaped_train.T,X_val.values.T,y_reshaped_val.T,101,'cross_entropy',decay_rate=0.001,rst=0.15,decay_loop=25)
print(f"Accuracy on test data with Tanh activation function: {model.accuracy(X_test.values.T, y_reshaped_test.T, 'cross_entropy')}")

#train and validation loss over epoch
plt.plot(history['train_loss'], label='Training Loss')
plt.plot(history['val_loss'], label='Validation Loss')
plt.title('Tanh: Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

#training and alidation accuracy over epoch
plt.plot(history['train_acc'], label='Training Accuracy')
plt.plot(history['val_acc'], label='Validation Accuracy')
plt.title('Tanh: Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# ReLU 
layer_dim=[512,64,32,num_labels]
activation=['leaky_relu','leaky_relu','leaky_relu','sigmoid']
model=ANN_scratch(8,4,num_labels,layer_dim,activation,batch_size=128,learning_rate=0.005,gradient_clip=0.3)
history=model.train(X_train.values.T,y_reshaped_train.T,X_val.values.T,y_reshaped_val.T,151,'cross_entropy',decay_rate=0.05,rst=0.7,decay_loop=15)
print(f"Accuracy on test data with ReLU activation function: {model.accuracy(X_test.values.T, y_reshaped_test.T, 'cross_entropy')}")

#train and validation loss over epoch
plt.plot(history['train_loss'], label='Training Loss')
plt.plot(history['val_loss'], label='Validation Loss')
plt.title('ReLU: Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

#training and alidation accuracy over epoch
plt.plot(history['train_acc'], label='Training Accuracy')
plt.plot(history['val_acc'], label='Validation Accuracy')
plt.title('ReLU: Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""##Part D"""

#Random
layer_dim=[512,128,64,num_labels]
activation=['sigmoid','sigmoid','sigmoid','sigmoid']
model=ANN_scratch(8,4,num_labels,layer_dim,activation,batch_size=16,learning_rate=0.001,weight_init='random')
history=model.train(X_train.values.T,y_reshaped_train.T,X_val.values.T,y_reshaped_val.T,101,'cross_entropy',decay_rate=0.001,rst=0.1,decay_loop=75)
print(f"Accuracy on test data: {model.accuracy(X_test.values.T,y_reshaped_test.T,'cross_entropy')}")

#train and validation loss over epoch
plt.plot(history['train_loss'], label='Training Loss')
plt.plot(history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

#training and alidation accuracy over epoch
plt.plot(history['train_acc'], label='Training Accuracy')
plt.plot(history['val_acc'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

#Zero
layer_dim=[512,128,64,num_labels]
activation=['sigmoid','sigmoid','sigmoid','sigmoid']
model=ANN_scratch(8,4,num_labels,layer_dim,activation,batch_size=16,learning_rate=0.001,weight_init='zero')
history=model.train(X_train.values.T,y_reshaped_train.T,X_val.values.T,y_reshaped_val.T,101,'cross_entropy',decay_rate=0.001,rst=0.1,decay_loop=75)
print(f"Accuracy on test data: {model.accuracy(X_test.values.T,y_reshaped_test.T,'cross_entropy')}")

#train and validation loss over epoch
plt.plot(history['train_loss'], label='Training Loss')
plt.plot(history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

#training and alidation accuracy over epoch
plt.plot(history['train_acc'], label='Training Accuracy')
plt.plot(history['val_acc'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

#Constant
layer_dim=[512,64,64,num_labels]
activation=['sigmoid','sigmoid','sigmoid','sigmoid']
model=ANN_scratch(8,4,num_labels,layer_dim,activation,batch_size=512,learning_rate=0.001,weight_init='constant',onebynthweight_value=4,gradient_clip=1)
history=model.train(X_train.values.T,y_reshaped_train.T,X_val.values.T,y_reshaped_val.T,101,'cross_entropy',decay_rate=0.001,rst=0.1,decay_loop=75)
print(f"Accuracy on test data: {model.accuracy(X_test.values.T,y_reshaped_test.T,'cross_entropy')}")

#train and validation loss over epoch
plt.plot(history['train_loss'], label='Training Loss')
plt.plot(history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

#training and alidation accuracy over epoch
plt.plot(history['train_acc'], label='Training Accuracy')
plt.plot(history['val_acc'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""##Part E"""

#Random
layer_dim=[512,128,64,num_labels]
activation=['sigmoid','sigmoid','sigmoid','sigmoid']
model=ANN_scratch(8,4,num_labels,layer_dim,activation,batch_size=16,learning_rate=0.001,weight_init='random')
history=model.train(X_train.values.T,y_reshaped_train.T,X_val.values.T,y_reshaped_val.T,101,'cross_entropy',decay_rate=0.001,rst=0.1,decay_loop=75)
print(f"Accuracy on test data: {model.accuracy(X_test.values.T,y_reshaped_test.T,'cross_entropy')}")

#train and validation loss over epoch
plt.plot(history['train_loss'], label='Training Loss')
plt.plot(history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

#training and alidation accuracy over epoch
plt.plot(history['train_acc'], label='Training Accuracy')
plt.plot(history['val_acc'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

#Random
layer_dim=[128,64,32,num_labels]
activation=['sigmoid','sigmoid','sigmoid','sigmoid']
model=ANN_scratch(8,4,num_labels,layer_dim,activation,batch_size=16,learning_rate=0.001,weight_init='random')
history=model.train(X_train.values.T,y_reshaped_train.T,X_val.values.T,y_reshaped_val.T,101,'cross_entropy',decay_rate=0.001,rst=0.1,decay_loop=75)
print(f"Accuracy on test data: {model.accuracy(X_test.values.T,y_reshaped_test.T,'cross_entropy')}")

#train and validation loss over epoch
plt.plot(history['train_loss'], label='Training Loss')
plt.plot(history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

#training and alidation accuracy over epoch
plt.plot(history['train_acc'], label='Training Accuracy')
plt.plot(history['val_acc'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

#Random
layer_dim=[32,16,8,num_labels]
activation=['sigmoid','sigmoid','sigmoid','sigmoid']
model=ANN_scratch(8,4,num_labels,layer_dim,activation,batch_size=16,learning_rate=0.001,weight_init='random')
history=model.train(X_train.values.T,y_reshaped_train.T,X_val.values.T,y_reshaped_val.T,101,'cross_entropy',decay_rate=0.001,rst=0.1,decay_loop=75)
print(f"Accuracy on test data: {model.accuracy(X_test.values.T,y_reshaped_test.T,'cross_entropy')}")

#train and validation loss over epoch
plt.plot(history['train_loss'], label='Training Loss')
plt.plot(history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

#training and alidation accuracy over epoch
plt.plot(history['train_acc'], label='Training Accuracy')
plt.plot(history['val_acc'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""Checking params load and save function"""

print(f"Accuracy on test data: {model.accuracy(X_test.values.T,y_reshaped_test.T,'cross_entropy')}")
model.save_weights('param_save')

model=ANN_scratch(8,4,num_labels,layer_dim,activation,batch_size=64,learning_rate=0.001,weight_init='random')
print(f"Accuracy on test data: {model.accuracy(X_test.values.T,y_reshaped_test.T,'cross_entropy')}")

model.load_weights('param_save')
print(f"Accuracy on test data: {model.accuracy(X_test.values.T,y_reshaped_test.T,'cross_entropy')}")

"""We can see that our weights loaded successfully"""